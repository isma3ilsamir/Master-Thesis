Proximity Forest is an ensemble of decision trees that utilizes the eleven elastic distances from EE \cite{fawaz2020inceptiontime,fawaz2019deep}.
It was introduced as an addition to scalable time series classification, to offer a more scalable and accurate classifier than EE \cite{tan2020fastee}.
On one side, EE is an accurate classifier as it combines 11 NN-algorithms each using a different elastic measure, but it's training process is slow \cite{lines2015time,bagnall2017great}.
This goes back to the leave-one-out-cross-validation (LOOCV) used to optimize the parameters each used metric \cite{shifaz2020ts}.
Proximity Forest saves a lot of the computational cost by replacing parameter searches with random sampling \cite{fawaz2020inceptiontime,fawaz2019deepreview}.\newline
To learn a single proximity tree, a selection process is carried out selecting a random time series examplar per class \cite{lucas2019proximity}.
Unlike conventional decision trees that use feature values for their nodes, proximity trees build their nodes using the selected examplars \cite{lucas2019proximity}.
For internal nodes, along with the examplar a randomly selected elastic distance measure ,from EE's 11 distances measures, exists. The parameters of the measure are randomly selected as well.
At each node, the examplars are compared to all training set instances using the selected distance measure and the data is split based on the proximity
of the training instances to the examplars. This process is repeated untill all leaves are pure \cite{lucas2019proximity}.
