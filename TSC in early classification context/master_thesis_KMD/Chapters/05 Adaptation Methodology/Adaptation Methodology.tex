\chapter{Framework and Experimental Design}
\label{ChapterFrameworkExperiment}
This chapter describes our proposed framework which compares TSCAs in a context that is inspired by the problem of eTSC and the design of our experiment.
We begin by defining what we mean by early classification context and how does our framework simulate it.
Then we discuss the structure of the framework, the software tools that were used to build it and how to evaluate it.
In the second section of the chapter we represent our experimental setup.

%%%%
%%%%There are three research questions that are discussed:
%%%%\begin{itemize}
  %%%%\item How to adapt existing TSC workflows for the early TSC context ?
  %%%%\item How to evaluate classifiers in the early TSC context ?
  %%%%\item How to evaluate the proposed solution ?
%%%%\end{itemize}
%

\section{The Framework}
\label{SectionNewFramework}
%% Intro
As motivated earlier, eTSC is a field which is concerned with classification of time series data with earliness and accuracy
as the main objectives. Dedicated eTSCAs focus on building models that can learn class labels of the data as early as possible
while maintaining accuracy \cite{mori2017early}.
Our framework, on the other hand, investigates the adaptation of the context in which conventional TSCA operate to learn a specific
classification problem and the effect it has on the classifiers' performance.

\subsection{Early Classification Context}
\label{SubsectionEarlyClassificationContext}
We define our early classification context as the problem in which a TSCA is trained on labeled but incomplete time series data, then
used to classify data instances of full length.

The motivation behind experimenting with such a context is driven by the medical field, where sometimes patients have to go through strenuous tests to be diagnosed.
This is specially the case in audiological examinations; as patients are exposed to signals which pushs the limits of their hearing discomfort to be able to examine their problems.
A classifier which can attain comparable reasults while only learning on a fraction of the whole time series data would shorten this process and thus relief patients from their discomfort.

To answer our first research question \textbf{\textit{How can we adapt existing ts classification workflow for the early TS scenario ?}}

We propose a framework that simulates an early classification context and runs a series of experiments using different TSCAs.
It divides whole timeseries data instances into smaller subseries using a chopping algorithm, then starts multiple learning processes.
In each learning process, the data is exposed incrementally to the classifier by revealing the next sub-series.
These processess simulate an incremental learning scenario, where the classifier is not exposed to all the data points collected from the beginning to the end at once.
However in each learning process it gets to know more information about the data instances than the previous process.
In order to assess the performance of classifiers that were exposed to less data points, we use the performance of classifiers that were exposed to full length training data as a baseline.
In the next subsection \ref{SubsectionFrameworkStructureToolkits} we discuss the the details and the structure of the framework.


\subsection{Structure and Toolkits}
\label{SubsectionFrameworkStructureToolkits}
% Libraires we use
We implemented our framework in python language.
For the implementation of the non-deep learning algorithms we used the open source libraires $sktime V. 0.5.3$ \cite{loning2019sktime} and $pyts V. 0.11.0$ \cite{JMLR:v21:19-763},
while for the implementation of the deep learning algorithm InceptionTime we used the library $sktime\-dl V .0.2.0$ which provides an interface for the implementation provided
in the study \cite{fawaz2020inceptiontime}. Finally, we used the library $Tslearn V. 0.5.0.5$ \cite{JMLR:v21:20-091} as a mediator between the different model implementations; due to it's
capacity to transform the raw data into any needed input format for all the other libraires.

The framework can be divided into several consecutive steps:
\begin{enumerate}
    \item User input
    \item Experiment preparation
    \item Experiment Execution
    \item Analysis report
\end{enumerate}

\subsubsection{User Input}
\label{SubsectionUserInput}
% User Input
In the user input step, the user provides the necessary information required by the framework to start it's experiments.
These information provide the guidelines of the process which will be carried out and decide what steps should be done in the
experiment preparation step.
Table \ref{TableUserInput} shows a summary of each of the needed information, represented as a parameter for the framework.

\begin{table}[hbt!]
  \setlength\extrarowheight{2pt} % for a bit of visual "breathing space"
  \begin{tabularx}{\textwidth}{|X|X|X|}
  \hline
  \textbf{Parameter} & \textbf{Data Type} & \textbf{Description} \\ \hline
    Dataset         & String List        & The name of the data set(s)                        \\ \hline
    Splits          & Integer            & The number of splits to apply on the data set      \\ \hline
    Hyperparam      & Boolean            & Learn hyperparameters for the model or skip        \\ \hline
    NumIterations   & Integer            & Number of hyperparameters combinations to sample   \\ \hline
    ChunksToKeep    & Integer List       & The indexes of the chunks to use                   \\ \hline
    FromBeg         & Boolean            & Reveal data chunks from beginning or end           \\ \hline
    ScoringFunction & String             & The strategy for evaluating the model on test data \\ \hline
  \end{tabularx}
  \caption{Input parameters required for using the framework}
  \label{TableUserInput}
\end{table}

We implemented the user input interface using $docopt$ \footnote{http://docopt.org}; a library which offers command-line interfaces through
the use of simple arguments and elements. It also allows to set default values for parameters which can be overriden if a value is passed.
In order to start running the framework, one provides the necessary parameter values through a simple terminal command.
Listing \ref{ListingUserInput} shows a sample command for running our master python file $run.py$ on the Computers data set using hyperparameters optimization,
while applying a split into 10 chunks.

\lstset{basicstyle=\ttfamily\small}
\begin{lstlisting}[language=Comsol,caption={Sample command for providing user input to the framework},captionpos=b,label={ListingUserInput}]
  python run.py --etsc --dataset=Computers --cv --split=10
\end{lstlisting}

\subsubsection{Experiment Preparation}
\label{SubsectionExperimentPreparation}
% Experiment Preparation
In the experiment preparation step, the framework starts preparing the different components of the experiment to be run.
These components include data set management, models initialization, preparation of the early classification context
and configuration of the training and testing processes.

% Datasets
Our framework's data sets handling module is configured to process data sets in either of two ways; automatic download from the UCR/UEA data archives,
or manually provided data sets that comply with some specifications.
Since the data loading module from $pyts$ \cite{JMLR:v21:19-763} offers integrated tools with the UCR/UEA data archives for fetching and downloading data,
we have configured our data module to extend $pyts$ by transparently checking both data archives when provided with a data set name.
The other option would be to provide a data set folder holding the name of the data set.
Inside the folder there should exist one training data set file with the name \enquote{FolderName\_TRAIN} and one testing data set file with the name \enquote{FolderName\_TEST}.
Which is the case for most of the data sets from both archives.
For multivariate data sets, our framework assumes that the training data set file, as well as the testing data set file, contains data of all dimensions.
For maximum compatibility, our framework extends the data processing utilities from $sktime$ \cite{loning2019sktime}
which can read data sets that comply with the guidelines provided by the UCR and the UEA archives.
We consider \enquote{arff} format as the primary expected data type.

%Multivariate (models initialization)
As mentioned earlier in section \ref{GreatBakeoffMultivariate}, there are two ways to work on multivariate time series data sets \cite{ruiz2020great}.
The first way is by using bespoke multivariate classifiers which can deal with the multiple dimensions available in the data.
While the other is by using ensembles of univariate classifiers, each fit to a separate dimension under the assumption of independence between the dimensions.
Our framework applies the second technique for handling multivariate data sets.

In the experiment preparation step, the framework checks the data and determines the number of dimensions in consists of.
If the data set is univariate then the model initalizes the default classifer implementations provided by the libraries.
In case the data set was found to be multivariate, we initialize a special classifier type which extends the usage of univariate classifiers for multivariate problems.
This special classifier is reffered to as $ColumnEnsembleClassifier$ in the $sktime$ library implementation and as the $MultivariateClassifier$ in $pyts$.

There are two assumptions that we make at this step in our framework.
First, we assume that to extend a specific univariate classifier to a problem, we should fit the same type of the classifer on all dimensions of the data.
This is goes back to our goal, that we want to compare classifiers from different groups and not to create an ensemble which makes use of different classifiers
applying different techniques, like HIVE-COTE does.
Second, we assume that we always have to fit one classifier per dimension, which have proved to be not a feasible solution for high dimensional data sets.
We will discuss this in more details in our results.

% Hyperparam + Scoring Function (configure training and testing)
In preparation for the following steps in the framework, the experiment preparation step plays a role in configuring the training and the testing processes.
For the training process, it is possible to either run using the default configuration of the classifiers from their libraries, or to learn it's hyperparameters through cross validation.
We use a randomized search over the hyperparameters space of classifiers.
The framework parameter $NumIterations$ represents the number of random samples to consider when optimizing the hyperparameters.
If the space of the hyperparameters is less than $NumIterations$, then the space is exhausted by applying a grid search. We have considered 5-folds cross validation for all experiments.

For the testing process, the framework allows the usage of any chosen performance metric which is supported by $sklearn$ library \cite{scikit-learn}.
The chosen performance metric is utilized for the evaluation during both; the cross validation process and for calculating performance on the test data set.
We consider \enquote{balanced accuracy} as our default performance metric; to account for problems where data sets have imbalanced classes distribution.

% Chopping Algorithm
Based on the assumption that all data instances are of the same length, we implement our early classification context using a time series chopping algorithm.
The algorithm is provided with some time series data set $T$ with instances of length $L$ and three user defined parameters; $s$ which decides the number of splits $L$ should be split to,
$FromBeg$ a flag which indicates whether the data chopping should occur from the beginning or the end of the time series $T$, and $ChunksToKeep$ which provides the user
with the option to use only specific chunks of interest and exclude the processing for all other chunks.
Algorithm \ref{AlgorithmChopping} demonstrates how the chopping algorithm works.

The result of the algorithm is then used to create new copies of $T$; $[T_{1}, T_{2}, \ldots, T_{s}]$, each copy $T_{i}$ containing all instances from $T$ but revealing a subsequence of $L$
which we call the chunk. A chunk is a sequence of $p$ data points starting from the beginning or the end of the instances in $T$ based on the value of the parameter $FromBeg$.
Each data set copy $T_{i+1}$ reveals one more chunk than the previous data set $T_{i}$ and the data set $T_{s}$ represents the set with instances of the full length $L$.
It should be noted that our algorithm tries to have an equal value for $p$ used for each chunk, but this is limited by the length of the time series and the provided number of splits.
The resulting data sets are then passed to the training process, where each algorithm applies it's own technique to learn on the data set.

\begin{algorithm}
    \caption{The Chopping Algorithm}\label{AlgorithmChopping}
    \begin{algorithmic}[1]
      \Function{$GetSplitIndexes$}{$T,s,ChunksToKeep$}
        \State $L \gets ExtractLength(T)$
        \State $ChunksSizes \gets GetChunkSizes(L,s)$
        \State $SplitIndexes \gets CumSum(ChunksSizes)$
        \If{$KeepChunks$ is not Null}
                \State $FilteredSplitIndexes \gets KeepChunks(SplitIndexes,ChunksToKeep)$
            \Else
                \State $FilteredSplitIndexes \gets SplitIndexes$
            \EndIf
        \State \textbf{return} $FilteredSplitIndexes$
      \EndFunction
    \end{algorithmic}
\end{algorithm}

To better understand how the chopping algorithm works, consider the next example.
Let's assume we have a data set $T$ with instances of length $L$ = 100, we set the value of $s$= 10 and $FromBeg$ = True.
The algorithm starts by calculating the value of $p$ for each of the required 10 chunks using the function \ref{FunctionChunkSizes}.

\begin{algorithm}
    \caption{Function to Get Chunks Sizes}\label{FunctionChunkSizes}
    \begin{algorithmic}[1]
      \Function{$GetChunkSizes$}{$L,s$}
      \State $ChunksSizes \gets$ [ ]
        \For{\texttt{$i \gets 0$ to $s-1$}}
            \If{$i < L\pmod{s}$}
                \State $p = (L \div{s}) + 1$
            \Else
                \State $p = L \div{s}$
            \EndIf
            \State $ChunksSizes.append(p)$
        \EndFor
        \State \textbf{return} $ChunksSizes$
      \EndFunction
    \end{algorithmic}
\end{algorithm}

The resulting list $ChunksSizes$ from the algorithm will contain the value 10 for each of the chunks ($ChunksSizes$ = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]);
this is because the length of the time series is divisible by the number of splits provided, so it results in perfect splits of the data and thus chunks of equal sizes.
If we assume the same scenario again but with a different $s$ value like 8, the algorithm will try to provide as equal values of $p$ as possible
giving back a $ChunksSizes$ of values [13, 13, 13, 13, 12, 12, 12, 12].

After calculating the chunk sizes, the chopping algorithm translates $ChunksSizes$ into a list of indexes called $SplitIndexes$.
Which is then used to create the different copies $T_{i}$ by selecting subsequences.
The values of $SplitIndexes$ are the indexes of the last time point that a specific chunk should read.
We calculate these values by carrying out a cumulative sum over the values of $ChunksSizes$.
For example, the list $ChunksSizes$ = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10] is translated into the list $SplitIndexes$ = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100].
This means that the corresponding data set $T_{1}$ will contain all it's data instances represented by the subsequences from the \nth{1} time point till the \nth{10} time points,
and the data set $T_{3}$ will contain subsequence from the \nth{1} time point till the \nth{30} time point.
When we apply the same translation to the list $ChunksSizes$ = [13, 13, 13, 13, 12, 12, 12, 12], it gets translated into $SplitIndexes$ = [13, 26, 39, 52, 64, 76, 88, 100].

Finally, the algorithm filters out the list $SplitIndexes$, if the user provided a list of specific chunks to keep using the variable $ChunksToKeep$.
The values passed through $ChunksToKeep$ corresponds to the indexes of the chunks the user is interested in, all the other chunks are excluded.
Note that this process is different from reducing the number of splits; as the number of splits affects the sizes of the chunks being created.
While $ChunksToKeep$ is used only to filter out the chunks after their sizes are already determined and translated into $SplitIndexes$.
The space of possible values for $ChunksToKeep$ is between [1, $s$]. The logic of the function is demonstrated in function \ref{FunctionChunksToKeep}

\begin{algorithm}
    \caption{Function to filter out Chunks}\label{FunctionChunksToKeep}
    \begin{algorithmic}[1]
      \Function{$KeepChunks$}{$SplitIndexes,ChunksToKeep$}
      \State $FilteredSplitIndexes \gets$ [ ]
        \For{\texttt{$i$ $\epsilon$ $ChunksToKeep$}}
                \State $FilteredSplitIndexes.append(SplitIndexes[i])$
        \EndFor
        \State \textbf{return} $FilteredSplitIndexes$
      \EndFunction
    \end{algorithmic}
\end{algorithm}

If we consider the list $ChunksToKeep$ = [1, 3, 5] for our example $SplitIndexes$ = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100].
The resulting filtered list will be $FilteredSplitIndexes$ = [10, 30, 50], which will result in only 3 copies of $T$; $T_{1}$, $T_{3}$ and $T_{5}$,
each represented by it's respective subsequence length.

% Relation between s and the earliness
If we have a closer look at the parameter $s$, we can recognise that it contributes to the granularity of the earliness factor that we are calculating.
The value of $s$ decides the total number of chunks we will use, which subsequently decides also the ratio represented by each chunk to the total length of the time series $T$.
That means the greater the value of $s$, the smaller the chunk sizes and consequently the more granular results we can test for the algorithms.
If we consider our previous example, when we set the value of $s$ = 10 then the length of each subsequence would be close to 10\% of the total length.
If we decided to change the value of $s$ = 20, then each chunk will roughly represents 5\% of the total length and so on.
Increasing the number of splits to be used comes with the cost of time and resources, each extra split adds one extra run per algorithm per data set.

\subsubsection{Experiment Execution}
\label{SubsectionExperimentExecution}
% Training Models
The experiment execution step is where the actual processing happens.
After the preparation of the chunks for the data sets in the previous step, learning processes start and each of the implemented classifiers starts training on the prepared data sets.
Each of these training processes is then followed by a testing process, where performance scores are calculated using the function provided by the parameter $ScoringFunction$ on testing data sets.

As mentioned in section \ref{SectionTSCA}, we follow the grouping criteria by \cite{bagnall2017great} which arranges TSCAs into 5 main categories based on their techniques.
We have included classifiers representing each of the 5 groups, in addition to a \nth{6} group which represents the family of deep learning time series algorithms.
We tried, as far as the implemented libraries allowed, to represent each group with 2 classifiers; one that is a non-ensebmle and another which is an ensemble.
A total of 10 classifiers are included in the framework covering all 6 groups, these can be broken down into the following:
\begin{itemize}
  \item Distance based : 1NN using MSM distance and PForest
  \item Phase dependent algorithms : TSF
  \item Shapelets: LS and ST
  \item Dictionary based : WEASEL and BOSS
  \item Deep learning : InceptionTime
  \item Classical baselines: 1NN using euclidean distance and 1NN using DTW
\end{itemize}

Table \ref{TableClassifierParams} shows the configuration and hyperparameters for all the included classifiers.
We used the same hyperparameter space like that of the original published papers as closely as possible.
During training, the number of hyperparameter sampling is decided by the framework parameter $NumIterations$ and we fix the number of cross validation folds to 5.

\begin{table}[hbt!]
  \setlength\extrarowheight{2pt} % for a bit of visual "breathing space"
  \begin{tabularx}{\textwidth}{|X|X|}
  \hline
  \textbf{Classifier} & \textbf{Parameters} \\ \hline
  MSM                 & c = \{0.01, 0.1, 1, 10, 100\}                                                        \\ \hline
  PForest             & k = 100 \newline r = 5                                                               \\ \hline
  TSF                 & r = 500 \newline splitting = \{entropy, gini\} \newline maxfeatures = \{sqrt, log2\} \\ \hline
  LS                  & $\lambda_{w}$ = \{0.01, 0.1, 1\} \newline
                        $L_{min}$ = \{0.025, 0.075, 0.125, 0.175, 0.2\} \newline R = \{1, 2, 3\} \newline
                        K = \{0.05, 0.15, 0.3\} \newline $\eta$ = 0.01 \newline
                        maxIter = \{2000, 5000, 10000\}                                                      \\ \hline
  ST                  & t = 60 mins \newline n=\{500,100\}                                                   \\ \hline
  CBOSS               & t = 60 mins \newline k=\{50, 100, 250, 500\} \newline s=250 \newline p=[0.5 - 1]     \\ \hline
  WEASEL              & ANOVA = \{True, False\} \newline bigrams =\{True, False\}
                        \newline binning = \{equi-depth, equi-width, information-gain\}                      \\ \hline
  Inception           & epochs = 1500 \newline batch size = 64 \newline $\eta$ = 0.001 \newline
                        kernel sizes = \{10, 20, 40\}                                                        \\ \hline
  DTW                 & full warping window                                                                  \\ \hline
  \end{tabularx}
  \caption{Parameters and configuration for TSCAs}
  \label{TableClassifierParams}
\end{table}

After the training processes are finished, the classifiers are then shown the testing data sets to evaluate their performance.
If the experiment is configured to do hyperparameter optimization,
then the classifier version that attained the highest validation score, from the 5 cross validation folds, is used for calculating the performance score on the testing data set.


\subsubsection{Analysis Report}
\label{SubsectionAnalysisReport}
% Analysis Report
The last step of our framework is the analysis report preparation.
After the successful running of the experiments and calculating performance scores using the provided scoring function, we create a single report for each run experiment.
We produce one report per classifier per data set.
We refer here by data set to each data set that was finally produced by the chopping algorithm.
These reports collect the essential data and statistics that form the basis of our results.
Table \ref{TableAnalysisReport} shows the structure of the output for the analysis report.

\begin{table}[hbt!]
  \setlength\extrarowheight{2pt} % for a bit of visual "breathing space"
  \begin{tabularx}{\textwidth}{|X|X|}
  \hline
  \textbf{Item} & \textbf{Description} \\ \hline
    Classifier                 & The classifier name                                             \\ \hline
    Train time                 & The total training time (CPU Time)                              \\ \hline
    Test time                  & The total testing time (CPU Time)                               \\ \hline
    Test score                 & Performance score on testing data set                           \\ \hline
    Params                     & List of hyperparameters used and their values                   \\ \hline
    Revealed \%                & Percent of data points used for training from the total length  \\ \hline
    Harmonic Mean              & Harmonic mean between test score and revealed \%                \\ \hline
    Data set                   & The name of the data set                                        \\ \hline
  \end{tabularx}
  \caption{Analysis report created for each experiment}
  \label{TableAnalysisReport}
\end{table}

Most of these information are already available once the testing process is finished; because either they are metadata about the experiment or metrics calculated during training and testing.
Metadata about the process is already known before the running of the process; like the $Classifier$, $Data set$ and $Revealed \%$.
While the processes metrics are $Train time$ , $Test time$; which represent the total duration of the learning and testing processes, and the $Test score$ which represents performance of the classifier.


There is only one metric which is calculated during the analysis report phase after all the other metrics are calculated; the harmonic mean.
We previously defined the HM in chapter \ref{EarlyTimeSeriesClassification}, it has been used by \cite{schafer2020teaser} as the objective function for evaluating their algorithm $TEASER$.
The HM is a popular choice for calculating a score that combines 2 objectives at the same time, which is in our case earliness and performance like the other eTSC problems.
Earliness is represented by the value of the parameter $Revealed \%$ which is a fraction of the full series length, e.g. 0.5 means 50\% of the total length.
On the other hand, performance is represented by the value of the parameter $Test score$.
We use HM as the means of evaluating classifiers in our experiment; due to it's capacity to incorporate both objectives in one score.
Which allows us to compare the trained classifiers in terms of a one score value without worrying about fixing the value of either the earliness or the performance.

\section{Experiments}
\label{SectionExperiment}
% Experiments
Our experiments were conducted on univariate data sets from the UCR archive as well as multivariate data sets from the UEA archive.
The data sets were chosen based on the criteria discussed in the subsection \ref{used data sets}.
Each of the data archives offers a train and test split of the data which we have used unchanged for all the classifiers.
All our experiments were run on a LINUX server with an AMD Ryzen 7 1700 Processor and 32GB RAM using Python 3.7.10.

We used the same experimental configuration through out all our experiments.
We fixed the value of the parameter $Splits$ = 10, that is for every data set the data would be revealed for the classifiers incrementally in batches of 10\% from the total length.
The chopping algorithm was set to always reveal data from the beginning of the time series.
Due to time limitations, we restricted our experiments to run only on the \nth{1}, \nth{2}, \nth{3} and \nth{10} chunksof the data sets by setting $ChunksToKeep$ = \{1,2,3,10\}.
We included the \nth{10} chunk to represent the baseline performance of each classifier if shown the full length data.
While the first 3 chunks were used to represent the classifiers in the early context scenarios.
Hyperparameters optimization was always carried out on all data sets with a $NumIterations$ maximum sampling value of 50 iterations, unless proven unfeasible for any of the classifiers due to memory shortage or time constraint.
In this case the experiment is repeated for all classifiers without hyperparameters optimization.
The selected performance metric for our experiments was $Balanced Accuracy$.
Balanced accuracy is a performance metric which can handle data sets with skewed class distributions by avoiding inflated performance estimates.
To calculate balanced accuracy, the $Recall$ value is computed for each class then averaged over the total number of classes.
We have set all our experiments to use the same configuration for all classifiers on the same data set.


\subsection{Excluded Classifiers}
\label{SubsectionExcludedClassifiers}
Some of the included classifiers in the framework were excluded from our experiments; either because they couldn't operate in the early classification context
or because they couldn't attain comparable results to the published performances by previously published frameworks.

% Models excluded due not handling the context
For example, KNNED and InceptionTime were excluded due to the nature of their techniques which couldn't handle our created context.
Both algorithms are clearly able to learn on chopped training data sets.
However once the testing phase is reached, they would fail to classify instances of the testing data,
showing errors related to mismatches between the expeccted length of instances and the provided length.
Yet they would finish the last chunk where the data is provided in it's original full length.
The reason why KNNED fails such scenario, is that it uses ED which is a point-wise comparison distance measure that cannot compare time series of unequal lengths \cite{tan2019time}.
On the other hand, InceptionTime is a deep learning model whose input layer architecture, represented by number of nodes, depends on the length of the input time series \cite{fawaz2019deepreview}.
Since we use full length instances for testing, this caused an overflow of input data than what the model structure was expecting.
There have been literature discussing adapting TSCAs to unequal time series, but this is out of the scope of our experiments.
For more details refer to \cite{caiado2009comparison, tan2019time, fawaz2019deepreview}


% Models excluded due to implementation error
Although KNNDTW has been a competent time series classifier for decades; thanks to the exploitation of the elastic distance measure DTW.
We couldn't get either implementation of KNNDTW from $sktime$ and $pyts$ to work on the chopped data; due to errors in the data representation needed by lower level internal libraries.
KNNDTW would have been able to operate in the early classification context if used with a full warping window; which would have been successful in handling extreme classification cases like classifying full length data
even when learning on the 10\% chunk data set.


% Models excluded due to performance
Two classifiers were excluded because they attained significantly inferior results compared to the published scores; these are KNNMSM and LS.
Specially on the InsectWingbeatSound data set, for which we attained a difference in performance of -45.71\% for KNNMSM and -20.71\% for LS than the results published by \cite{bagnall2017great}


% Models used
Our experiments proceeded with the remaining 5 classifiers