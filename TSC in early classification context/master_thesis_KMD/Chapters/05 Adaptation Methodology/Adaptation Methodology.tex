\chapter{The New Approach}
\label{ChapterNewApproah}
This chapter describes our proposed framework which compares TSCAs in a context which is inspired by the problem of eTSC.
We will define what we mean by early classification context and how do we simulate it. We will also discuss the different steps
of the framework and what tools were used to build it.

\section{The Framework}
\label{SectionNewFramework}
%% Intro
As motivated earlier, eTSC is a field which is concerned with classification of time series data with earliness and accuracy
as the main objectives. Dedicated eTSCAs focus on building models that can learn class labels of the data as early as possible
while maintaining accuracy \cite{mori2017early}.
Our framework, on the other hand, investigated the adaptation of the context in which conventional TSCA operate to learn a specific
classification problem and the effect it had on the classifiers' performance.

% Should describe the early classification context
We define our early classification context as the problem in which a TSCA is trained on labeled but incomplete time series data, then
used to classify data instances of full length.

The baseline performance for each classifier is determined by it's performance on the full length data.

% Describe the implementation
Based on the assumption that all data instances are of the same length, we implement our early classification context using a time series chopping algorithm.
The algorithm is provided with some time series data set $T$ with instances of length $L$ and three user defined parameters; $s$ which decides the number of splits $L$ should be split to,
$FromBeg$ a flag which indicates whether the data chopping should occur from the beginning or the end of the time series $T$, and $ChunksToKeep$ which provides the user
with the option to use only specific chunks of interest and exclude the processing for all other chunks.
The algorithm should return new copies of $T$; $[T_{1}, T_{2}, \ldots, T_{s}]$, each copy $T_{i}$ containing all instances from $T$ but revealing a subsequence of $L$
which we call the chunk. A chunk is a sequence of $p$ data points starting from the beginning or the end of the instances in $T$ based on the value of the parameter $FromBeg$.
Each data set copy $T_{i+1}$ reveals one more chunk than the previous data set $T_{i}$ and the data set $T_{s}$ represents the set with instances of the full length $L$.
It should be noted that our algorithm tries to have an equal value for $p$ used for each chunk, but this is limited by the length of the time series and the provided number of splits.
The resulting data sets are then passed to the training process, where each algorithm applies it own technique to learn on the data set.

To better understand how the chopping algorithm works, consider the next example.
Let's assume we have a data set $T$ with instances of length $L$ = 100, we set the value of $s$= 10 and $FromBeg$ = True.
The algorithm starts by calculating the value of $p$ for each of the required 10 chunks using the function \ref{FunctionChunkSizes}.

\begin{algorithm}
    \caption{Function to Get Chunks Sizes}\label{FunctionChunkSizes}
    \begin{algorithmic}[1]
      \Function{$GetChunkSizes$}{$L,s$}
      \State $ChunksSizes \gets$ [ ]
        \For{\texttt{$i \gets 0$ to $s-1$}}
            \If{$i < L\pmod{s}$}
                \State $p = (L \div{s}) + 1$
            \Else
                \State $p = L \div{s}$
            \EndIf
            \State $ChunksSizes.append(p)$
        \EndFor
        \State \textbf{return} $ChunksSizes$
      \EndFunction
    \end{algorithmic}
\end{algorithm}

The resulting list $ChunksSizes$ from the algorithm will contain the value 10 for each of the chunks ($ChunksSizes$ = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]);
this is because the length of the time series is divisible by the number of splits provided, so it results in perfect of splits of the data and thus chunks of equal sizes.
If we assume the same scenario again but with a different $s$ value like 8, the algorithm will try to provide as equal values of $p$ as possible
giving back a $ChunksSizes$ of values [13, 13, 13, 13, 12, 12, 12, 12].

After calculating the chunk sizes, the chopping algorithm translates $ChunksSizes$ into a list of indexes called $SplitIndexes$.
Which is then used to create the different copies $T_{i}$ by selecting subsequences.
The values of $SplitIndexes$ are the indexes of the last time point that a specific chunk should read.
We calculate these values by carrying out a cumulative sum over the values of $ChunksSizes$.
For example, the list $ChunksSizes$ = [10, 10, 10, 10, 10, 10, 10, 10, 10, 10] is traslated into the list $SplitIndexes$ = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100].
This means that the corresponding data set $T_{1}$ will contain all it's data instances represented by the subsequences from the \nth{1} time point till the \nth{10} time points,
and the data set $T_{3}$ will contain subsequence from the \nth{1} time point till the \nth{30} time point.
When we apply the same translation to the list $ChunksSizes$ = [13, 13, 13, 13, 12, 12, 12, 12], it gets translated into $SplitIndexes$ = [13, 26, 39, 52, 64, 76, 88, 100].

Finally, the algorithm filters out the list $SplitIndexes$, if the user provided a list of specific chunks to keep using the variable $ChunksToKeep$.
The values passed through $ChunksToKeep$ corresponds to the indexes of the chunks the user is interested in, all the other chunks are excluded.
Note that this process is different from reducing the number of splits; as the number of splits affects the sizes of the chunks being created.
While $ChunksToKeep$ is used only to filter out the chunks after their sizes are already determined and translated into $SplitIndexes$.
The space of possible values for $ChunksToKeep$ is between [1, $s$]. The logic of the function is demonstrated in \ref{FunctionChunksToKeep}

\begin{algorithm}
    \caption{Function to filter out Chunks}\label{FunctionChunksToKeep}
    \begin{algorithmic}[1]
      \Function{$KeepChunks$}{$SplitIndexes,ChunksToKeep$}
      \State $FilteredSplitIndexes \gets$ [ ]
        \For{\texttt{$i$ $\epsilon$ $ChunksToKeep$}}
                \State $FilteredSplitIndexes.append(SplitIndexes[i])$
        \EndFor
        \State \textbf{return} $FilteredSplitIndexes$
      \EndFunction
    \end{algorithmic}
\end{algorithm}

If we consider the list $ChunksToKeep$ = [1, 3, 5] for our example $SplitIndexes$ = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100].
The resulting filtered list will be $FilteredSplitIndexes$ = [10, 30, 50], which will result in only 3 copies of $T$; $T_{1}$, $T_{3}$ and $T_{5}$,
each represented by it's respective subsequence length.

% Relation between s and the earliness
If we have a closer look at the parameter $s$, we can recognise that it contributes to the granularity of the earliness factor that we are calculating.
The value of $s$ decides the total number of chunks we will use, which subsequently decides also the ratio represented by each chunk to the total length of the time series $T$.
That means the greater the value of $s$, the smaller the chunk sizes and consequently the more granular results we can test for the algorithms.
If we consider our previous example, when we set the value of $s$ = 10 then the length of each subsequence would be close to 10\% of the total length.
If we decided to change the value of $s$ = 20, then each chunk will roughly represents 5\% of the total length and so on.
Increasing the number of splits to be used comes with the cost of time and resources, each extra split adds one extra run per algorithm per data set.



This cutting mechanism controls is what decides the notion of earliness in our calculations.
