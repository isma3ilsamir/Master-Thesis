\chapter{Related Work}
\label{ChapterRelatedWork}
The main contribution of this thesis is to provide a framework that compares TSCAs in an early classification context.
The idea of creating frameworks to standardize the process of comparison between algorithms is not new to the time series data domain.
It was actually needed due to the increasing interest in time series data and the vast amount of newly introduced algorithms.
In this chapter we will briefly discuss 3 other frameworks that had been developed to compare TSCA on univariate and multivariate data sets.

\section{The great time series classification bake off}
\label{GreatBakeoffUnivariate}
One of the most famous studies in comparing TSCAs is \cite{bagnall2017great} and which has inspired some of the work done in this thesis.
At the time when this paper was published, the TSC domain had already witnessed hundreds of algorithms being proposed, each claiming superior performance.
There was no clear structure, at the time, for experimenting and benchmarking algorithms' performance, which pushed for having a more concrete structure to be followed in succeeding research.
The primary goal of the experiment was to assess the average accuracy of the different classifiers on a broad set of univariate data sets.
The Scalability and Efficiency of the classifiers were considered as secondary goals.

\subsection{Inclusion Criteria}
\label{subsectionUniBakeoffInclusion}
The experiment collected 18 different classifiers to be compared with 2 baseline classifiers on the UCR data archive (discussed in \ref{UCR}).
Algorithms were selected based on three criteria; an algorithm had to be published in a high impact conference or journal, used some data sets from the UCR archive to assess performance
and the availability of the code.

In order to better describe the various techniques applied by the algorithms and what differences characterize each of them, they formulated a taxonomy which 
groups algorithms based on the features they try to find to discriminate between different classes.
These groups were; whole series similarity (also referred to as distance-based algorithms), phase dependent intervals, phase independent intervals (shapelets), dictionary based and ensemble algorithms.
We follow the same taxonomy for our experiment as well and describe these groups in more details in section \ref{SectionTSCA}.

\subsection{Experimental Design}
\label{subsectionUniBakeoffExperiment}
The framework ran 100 resampling folds for each of the data sets for each classifier, totalling 8500 (85 data sets $\times$ 100 resamples) experiments for each classifier.
The first run used the default train/test split provided with the data set, while the remaining resamples were stratified to preserve the class distribution in the default split.
Using the same parameters as the relevant published papers, the experiment limited the number of parameter values for each classifier to a maximum of 100.
They used different number of cross validation folds based on the type of algorithm. LOOCV was used for algorithms utilizing distance based measures,
while for the other classifiers a new model was created for each parameter set. Resulting in a total of 850,000 models per classifier.

\section{Deep learning for time series classification: a review}
\label{DeepLearningReview}
Another paper that focused on comparison of deep learning algorithms for TSC was \cite{fawaz2019deepreview}.
Deep learning algorithms had already proved competency in many application fields including image classification, speech recognition and natural language processing \cite{he2016deep,santos2016literature,krizhevsky2012imagenet,guan2019towards}
and was gaining more popularity in TSC problems \cite{zheng2014time,zheng2016exploiting,zhao2017convolutional}.
The main goals were to benchmark the performance of deep learning algorithms on TSC problems; as they were less studied than the other algorithms and to provide an open source framework for deep learning on TSC.

\subsection{Inclusion Criteria}
\label{subsectionDeepLearningReviewInclusion}
The framework included 9 discriminative model end-to-end deep learning algorithms developed to work on TSC problems.
They started by grouping deep learning algorithms into two main groups; generative and discriminative approaches.

Generative models involve an unsupervised step which tries to learn a quality representation of the time series, then this new representation is fed to another classifier in a learning phase.
But this family of algorithms was excluded due to it's complexity and incompetence compared to the other group \cite{le2017time,bagnall2017great}.

As for the discriminative models, algorithms that needed preprocessing of features prior to learning were also excluded; to avoid the bias of hand crafted features.
The remaining discriminative approaches were; Multi Layer Perceptron (MLP), Fully Convolutional Neural Network (FCN), Residual Network (ResNet), Encoder, Multi-scale Convolutional Neural Network (MCNN),
Time Le-Net, Multi Channel Deep Convolutional Neural Network (MCDCNN), Time Convolutional Neural Network (Time-CNN) and Time Warping Invariant Echo State Network (TWIESN).

These 9 algorithms were to be compared to each other and to 7 other classifiers.
A group consisting of the best 4, out of the 18, classifiers that were included in the experiment by \cite{bagnall2017great} which are; Elastic Ensemble (EE), Bag Of SFA Symbols (BOSS), Shapelet Transform (ST), Collective of Transformation-based Ensemble(COTE).
They have also included Hierarchical Vote Collective of Transformation-Based Ensembles (HIVE-COTE), which is an extension of COTE using hierarchical voting and two extra classifiers.
In addition to Proximity Forest, an ensemble based on the same concept of Random Forests but using class examplars instead of feature split values.
Finally, the classic 1-NN classifier utilizing Dynamic Time warping (DTW) elastic distance measure with the warping window set through cross-validation on training data set.
Many of these classifiers we cover later on in section \ref{SectionTSCA}.

\subsection{Experimental Design}
\label{subsectionDeepLearningReviewExperiment}
The experiment covered both univariate data sets and multivariate data sets. The univariate data sets were represented by the same 85 z-normalized data sets from the UCR archive as \cite{bagnall2017great}.
As for the multivariate data sets, 12 data sets from the archive by Mustafa Baydogan\footnote{http://www.mustafabaydogan.com/multivariate-time-series-discretization-forclassification.html}
were used. Due to the existence of instances with unequal lengths in the multivariate archive, they used linear interpolation, suggested by \cite{ratanamahatana2005three},
such that all instances inside the same data set are adjusted to the length of the longest instance. Z-normalization was not applied to any of the Baydogan archive.

In order to avoid bias from the initial weights assigned to the classifiers, for each of the data sets for each classifier, the framework used 10 runs for training and then considered the mean accuracy of all runs together.
The original train/test split was used in all 10 runs, but the initial weights were randomized.
The framework applied optimization for the hyperparameters of the deep learning algorithms, but not for the other classifiers.
With the exception of TWIESN, the number of epochs used during optimization ranged between 100 and 5000.
A model checkpoint procedure was involved. This meant that after training a model for 1000 epochs, the model which attains the least error on the validation data set is chosen for evaluation.
All the models were initialized randomly using the methodology from \cite{glorot2010understanding} and were optimized using variants of stochastic gradient descent; like Adam \cite{kingma2014adam} and AdaDelta \cite{zeiler2012adadelta}.
For FCN, MLP, and ResNet if the training loss had not improved for 50 consecutive epochs, then learning rate was decreased by 0.5 to a minimum of 0.0001.

\section{The great multivariate time series classification bake off}
\label{GreatBakeoffMultivariate}
A recent study was carried out by \cite{ruiz2020great} focusing on comparing multivariate TSCAs.
With previous research paying more attention to univariate classifiers and univariate problems, multivariate time series classification (MTSC) recieved less attention.
Which meant that the MTSC domain is now in the same position that univariate TSC had been in some years ago and there is a need for a benchmarking framework to
guide upcoming research on how to compare multivariate TSCAs to already existing ones.

MTSC is simply the classifiation of time series data collected, where multiple features are collected at each time point and a single label is assigned.
MTSC is more challenging than univariate TSC because the discriminative features can be in the interaction between the multiple dimensions
and not only based on the interdependency of consecutive values in one dimension.
The framework inlcuded two techniques that algorithms follow to deal with multivariate data; either dedicated multivariate TSCAs which are, by design, able to handle multiple dimensions,
or adapted univariate classifiers which can handle multivariate time series instances.
The adapted univariate classifiers technique is based on two pillars; assuming independence between the dimensions of the data and fitting separate
univariate classifiers to each dimension then ensemble their results. We used this technique in our experiment for handling multivariate data sets.

\subsection{Inclusion Criteria}
\label{subsectionMultiBakeoffInclusion}
Although univariate and multivariate TSC problems differ in their complexity, but the algorithms for both can be grouped, based on the technique they apply, in the same way.
These are; distance based similarity, phase dependent intervals, phase independent intervals (shapelets), dictionary based and deep learning algorithms.

The framework collected 16 classifiers from these different groups and compared them on the 30 data sets from the UEA multivaraite data archive (discussed in \ref{UEA}).
The included classifiers had to meet 2 simple criteria; code availability and runnable code.
The classifiers encompassed 3 variations of the classic DTW which represented the baseline; DTW using Independent Warping ($DTW\textsubscript{I}$) which calculates DTW distance for each dimension separately then
sums all distances, DTW using Dependent Warping ($DTW\textsubscript{D}$) which simultaneously calculates the DTW distance across all dimensions for each time point and DTW using Adaptive Warping
($DTW\textsubscript{A}$) which adapts to each instance by calculating both $DTW\textsubscript{I}$ and $DTW\textsubscript{D}$ and then chooses between them based on a threshold score.
The multivariate classifiers were; Generalized random shapelet forest (gRFS), WEASEL+MUSE, Canonical interval forest (CIF), Random Convolutional Kernel Transform (ROCKET),
Multiple Representation Sequence Learner (MrSEQL), Residual network (ResNet), InceptionTime, Time series attentional prototype network (TapNet).
The adapted univariate classifiers were; the ensemble HIVE-COTE and each of it's single components; Shapelet Transform (ST), Time Series Forest (TSF), Contractable Bag of Symbolic Fourier Approximation Symbols (CBOSS)
and Random Interval Spectral Ensemble (RISE).
Two specialized toolkits in time series machine learning were used for the experiment. One of them is python based, while the other is java based.
Most of the algorithms are implemented in both, but few exist only in one. More information is available in \cite{ruiz2020great}.

\subsection{Experimental Design}
\label{subsectionMultiBakeoffExperiment}
The framework ran 30 resamples for each data set for each classifier to calculate performance metrics, the first run using the default train/test split of the data set, while the remaining 29 runs using
stratified sampling; to maintain the class distribution of the original split.
In contrast to the previous experiments where only accuracy was measured, this experiment had other measures beside which are; area under the ROC, balanced accuracy, F1, negative log likelihood,
Matthew’s correlation coefficient, recall/sensitivity, precision and specificity. Still accuracy was the primary performance measure used in comparisons; due to it's easier interpretation.

The classifiers were initialized using the default structure and hyperparameters in their original published papers.
Apart from the classifiers that have internal tuning processes, no other external tunings were applied.
Even for the window size of DTW, which is a was proved to have a small but significant improvement of performance for univariate data sets \cite{ratanamahatana2005three}.
Using a side experiment, where an untuned DTW was compared to a naive implementation of DTW with window sizes between 0\% and 100\% on 21 of the data sets.
The experiment had proved that the untuned DTW was better on 14 data sets, but with no significant difference between both.
Which meant that tunning was not needed as far as the experiment is concerned.

The data sets of the UEA archive are not normalized and were presented to the classifiers without any preprocessing.
This might had caused a disadvantage for WEASEL+MUSE, which doesn't have an internal normalization process, against other classifiers like,
ROCKET; gRSF; TapNet; InceptionTime; ResNet; CBOSS; STC and CIF, which do.
The decision of excluding data normalization was based on another side experiment that they carried out.
A comparison was done using the three DTW variants, once on normalized data and once on non-normalized data.
The experiment had proved that data normalization had no significant effect on the performance of the classifiers, but overall the performance of DTW declined with normalization.
The same experiment was repeated for HIVE-COTE and it's components and the same result was observed.
This meant that data normalization was also not considered important for the scope of the experiment.

\section{Evaluation Criteria}
\label{ReviewsEvaluation}
All three frameworks follow the same methodology, introduced by \cite{demvsar2006statistical}, for comparing their classifiers over multiple data sets.
When using multiple resamples over data sets, the score of a classifier on a given data set is calculated as the average of it's accuracy over all resamples.
The methodology recommends using a Friedman ranking test to refute the null hypothesis; that there is no difference between the ranks of the classifiers.
After refuting the null hypothesis and applying ranks to each classifier, a pairwise post-hoc analysis is done according to the recommendation of \cite{benavoli2016should}.
Which applies a Wilcoxon signed rank test using a Holm's alpha correction of value 5\% to form cliques of classifiers.
Each clique represents a group of classifiers where there is no significance between their pairwise comparisons.
Results are described by a critical difference diagram, where the average rank of each classifier is noted and cliques are expressed by a thick line.