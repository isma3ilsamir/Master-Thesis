\chapter{Related Work}
\label{ChapterRelatedWork}
The main contribution of this thesis is to provide a framework that compares TSCAs in an early classification context.
The idea of creating frameworks to standardize the process of comparison between algorithms is not new to the time series data domain.
It was actually needed due to the increasing interest in time series data and the vast amount of newly introduced algorithms.
In this chapter we will briefly discuss 3 other frameworks that had been developed to compare TSCA on univariate and multivariate data sets.

\section{The great time series classification bake off}
\label{GreatBakeoffUnivariate}
One of the most famous studies in comparing TSCAs is \cite{bagnall2017great} and which has inspired some of the work done in this thesis.
At the time when this paper was published, the TSC domain had already witnessed hundreds of algorithms being proposed, each claiming superior performance.
There was no clear structure, at the time, for experimenting and benchmarking algorithms' performance, which pushed for having a more concrete structure to be followed in succeeding research.

The experiment collected 18 different classifiers to be compared with 2 baseline classifiers on the UCR data archive (discussed in \ref{UCR}).
Algorithms were selected based on three criteria; an algorithm had to be published in a high impact conference or journal, used some data sets from the UCR archive to assess performance
and the availability of the code.

The primary goal of the experiment was to assess the average accuracy of the different classifiers on a broad set of univariate data sets.
The Scalability and Efficiency of the classifiers were considered as secondary goals.

In order to better describe the various techniques applied by the algorithms and what differences characterize each of them, they formulated a taxonomy which 
groups algorithms based on the features they try to find to discriminate between different classes.
These groups were; whole series similarity, phase dependent intervals, phase independent intervals, dictionary based and ensemble algorithms.
We followed the same taxonomy for our experiment as well.

The framework ran 100 resampling folds for each of the data sets for each classifier, totalling 8500 (85 data sets $\times$ 100 resamples) experiments for each classifier.
The first run used the default train/test split provided with the data set, while the remaining resamples were stratified to preserve the class distribution in the default split.
Using the same parameters as the relvent published papers, the experiment limited the number of parameter values for each classifier to a maximum of 100.
They used different number of cross validation folds based on the type of algorithm. LOOCV was used for algorithms utilizing distance based measures,
while for the other classifiers a new model was created for each parameter set. Resulting in a total of 850,000 model per classifier.

\section{Deep learning for time series classification: a review}
\label{DeepLearningReview}
Another paper that focused on comparison of deep learning algorithms for TSC was \cite{fawaz2019deepreview}.
Deep learning algorithms had already proved competency in many application fields including image classification, speech recognition and natural language processing \cite{he2016deep,santos2016literature,krizhevsky2012imagenet,guan2019towards}
and was gaining more popularity in TSC problems \cite{zheng2014time,zheng2016exploiting,zhao2017convolutional}.
The main goal was to benchmark the performance of deep learning algorithms on TSC problems; as they were less studied than the other algorithms and provide an open source framework for deep learning on TSC.

The framework included 9 discriminative model end-to-end deep learning algorithms developed to work on TSC problems. They started by grouping deep learning algorithms into two main groups;
generative and discriminative approaches.
Generative models involve an unsupervised step which tries to learn a quality representation of the time series, then this new representation is fed to another classifier in a learning phase.
But this family of algorithms was excluded due to it's complexity and incompetence compared to the other group \cite{le2017time,bagnall2017great}.
As for the discriminative models, algorithms that needed preprocessing of features prior to learning were also excluded; to avoid the bias of hand crafted features.
The remaining discriminative approaches were; Multi Layer Perceptron (MLP), Fully Convolutional Neural Network (FCN), Residual Network (ResNet), Encoder, Multi-scale Convolutional Neural Network (MCNN),
Time Le-Net, Multi Channel Deep Convolutional Neural Network (MCDCNN), Time Convolutional Neural Network (Time-CNN) and Time Warping Invariant Echo State Network (TWIESN).

These 9 algorithms were to be compared to each other and to 7 other classifiers.
A group consisting of the best 4, out of the 18, classifiers that were included in the experiment by \cite{bagnall2017great} which are; Elastic Ensemble (EE), Bag Of SFA Symbols (BOSS), Shapelet Transform (ST), Collective of Transformation-based Ensemble(COTE).
They have also included Hierarchical Vote Collective of Transformation-Based Ensembles (HIVE-COTE), which is an extension of COTE using hierarchical voting and two extra classifiers.
In addition to Proximity Forest, an ensemble based on the same concept of Random Forests but using class examplars instead of feature split values.
Finally, the classic 1-NN classifier utilizing Dynamic Time warping (DTW) elastic distance measure with warping window set through cross-validation on training data set.
Many of these classifiers we cover in Chapter \ref{ChapterClassificationAlgorithms}

The experiment covered both univariate data sets and multivariate data sets. The univariate data sets were represented by the same 85 z-normalized data sets from the UCR archive as \cite{bagnall2017great}.
As for the multivariate data sets, 12 data sets from the archive by Mustafa Baydogan\footnote{http://www.mustafabaydogan.com/multivariate-time-series-discretization-forclassification.html}
were used. Due to the existence of instances with unequal lengths in the multivariate archive, they used linear interpolation, suggested by \cite{ratanamahatana2005three},
such that all instances inside the same data set are adjusted to the length of the longest instance. Z-normalization was not applied to any of the Baydogan archive.

In order to avoid bias from the initial weights assigned to the classifiers, for each of the data sets for each classifier, the framework used 10 runs for training and then considered the mean accuracy of all runs together.
The original train/test split was used in all 10 runs, but the initial weights were randomized.
The framework applied optimization for the hyperparameters of the deep learning algorithms, but not for the other classifiers.
With the exception of TWIESN, the number of epochs used during optimization ranged between 100 and 5000.
A model checkpoint procedure was involved. This meant that after training a model for 1000 epochs, the model which attains the least error on the validation data set is chosen for evaluation.
All the models were initialized randomly using the methodology from \cite{glorot2010understanding} and were optimized using variants of stochastic gradient descent; like Adam \cite{kingma2014adam} and AdaDelta \cite{zeiler2012adadelta}.
For FCN, MLP, and ResNet if the training loss had not improved for 50 consecutive epochs, then learning rate was decreased by 0.5 to a minimum of 0.0001.

\section{The great multivariate time series classification bake off}
\label{GreatBakeoffMultivariate}

\section{Evaluation Criteria}
\label{ReviewsEvaluation}
For the evaluation of differences in accuracy,


