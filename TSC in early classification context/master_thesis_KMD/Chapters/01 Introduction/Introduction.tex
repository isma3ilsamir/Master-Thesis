\chapter{Introduction}
\label{ChapterIntroduction}
Time Series Classification (TSC) is a research field which has grabbed a lot of attention in the data mining community during the last decade.
This goes back to the fact that time series data exists, by nature, in numerous scenarios in our life; medical examination records of patients \cite{griffin2001toward,ghalwash2012early}, consumption of electrical devices in smart homes and smart grids \cite{gao2014plaid,10.1145/2611286.2611333} and astronomy \cite{protopapas2006finding,rakthanmanon2013fast} are only few examples of them.

The problem of time series data classification has been tackled in different ways and motivated by vairous objectives.
When the main driving objective is to attain high classification accuracy, then this is referred to simply as Time Series Classification (TSC).
TSC has also been extended to cover space and time complexity along with accuracy.
On the other hand, if time plays a critical role in the problem being tackled beside accuracy; like early detection of malfunctions in industrial processes or diagnosis of a patient with a life threatning disease.
In this case the classification problem is referred to as Early Time Series Classification (eTSC).

A lot of algorithms have been introduced to tackle the problem of TSC.
According to \cite{bagnall2017great}, these algorithms can be divided, based on their technique, into six groups.
Distance-based time series algorithms compare instances of time series, usually using 1-NN classifiers and an elastic distance measure between the data points of both time series.
Phase dependent interval algorithms operate by extracting informative features from intervals of time series, they are more suitable for long and noisy data than distance-based time series algorithms.
Phase independent interval algorithms are used when a class can be identified using a single or multiple patterns regardless of when they occur during the time series.
Dictionary based algorithms consider the number of repetitions of patterns as a factor of classification and not just simple occurence of one.
Ensembles combine the power of different algorithms, either of different or same core technique, then make the final classification decision based on voting.
In addition to the previous algorithms, there are also deep learning time series algorithms which build classifiers using generative as well as discriminative neural networks models.

On the other hand, Early Time Series Classification algorithms are designed to deal with less data in order to achieve earliness of prediction, without sacrificing accuracy as much as possible.
Since accuracy and earliness are contradicting by nature, eTSC's main goal is to find a tradeoff between both objectives \cite{gupta2020fault}.
Many of the techniques that have been applied in TSC have also been applied for eTSC.
Using 1-NN classifiers is a very popular TSC technique, which has been extended to eTSC by combining it with Mimimum Prediction Length (MPL)\cite{xing2012early}.
Phase independent intervals\cite{ghalwash2012early,he2015early}, generative and discriminative model based classifiers\cite{li2014early,cheng2018predicting} as well as deep learning algorithms \cite{huang2018multivariate, russwurm2019end}
have all been extended to tackle eTSC problems.


Both, Time Series Classification Algorithms (TSCA) and Early Time Series Classification Algorithms (eTSCA), have introduced well performing algorithms in terms of their respective performance measures.
The existence of comprehensive benchmark data archives \cite{UCRArchive2018,bagnall2018uea} has helped regulate and standardize the process of comparison between classifiers.
Algorithms can be benchmarked on a diverse set of datasets with different characteristics to avoid biased results and cherry picking.

Like other fields, the \emph{No free lunch theorem} exists in the TSC domain as well \cite{bagnall2017great,bostrom2018shapelet}.
This means that there is no one specific algorithm that can generalize to all problems and prevail over all other competing algorithms.
With that being the case, this leaves an open door for continuous investigation of how the performance of algorithms would change on different data sets and in different contexts.

Inspired by the of the problem of eTSC and its application in diverse areas.
We define a situation in which a classifer is put into test to learn on only a part of the available data; to see if it will be able to gracefully degrade.
We consider a classifier to be graceful, or fault tolerant, if it is able to maintain statistically significant results by learning on less data in comparison to its results when learning on full length data.
We refer to this situation as the early classification context.

The motivation behind experimenting with such a context is driven by the medical field, where sometimes patients have to go through strenuous tests to be diagnosed.
This is specially the case in audiological examinations; as patients are exposed to signals which pushes the limits of their hearing discomfort to be able to examine their problems.
A classifier which can attain comparable reasults while only learning on a fraction of the whole time series data would shorten this process and thus relief patients from their discomfort.

\subsubsection*{Goal of this Thesis}
\label{thesisGoals}
The goals of this master thesis can be addressed as:

\begin{itemize}
    \item Provide a solution for adapting existing time series workflows for an early classification context. The solution should run experiments on a group of data sets from the UCR/UEA archive and measure the performance of classifiers within the early classification context. By the end of the experiments the solution should collect the results and provide an analysis of the results, the analysis should:
    \begin{itemize}
        \item Identify if classifiers can sustain good performance while learning on fewer data points in comparison to learning on the full length
        \item Identify if there is a winning classifier across the executed experiments
        \item Identify the aspects of the data set which affect the performance of classifiers within the context. Such as train size, number of classes, number of dimensions, length, etc.
    \end{itemize}
    \item Identify a suitable evaluation criteria for classifiers in the proposed early classification context, trading classification accuracy against time series length.
    \item Evaluate the proposed solution for adapting the time series workflows to be used for future recommendations
\end{itemize}

\subsubsection*{Structure of the Thesis}
\label{thesisStructure}
The rest of the paper is structured as follows. In chapter \ref{ChapterClassificationAlgorithms}, we review the two main classification problems;
TSC and eTSC as well as their algorithms. We disuss each algorithm group and focus on the algorithms that we implemented in our framework.
In chapter \ref{ChapterRelatedWork}, we review 3 related frameworks to our work. These are frameworks that compare time series algorithms on the UCR and UEA data archives.
Two of these frameworks compare univariate algorithms, while the third compares multivariate algorithms.
In chapter \ref{ChapterFramework}, we describe the conceptual design and the implementation of our proposed framework.
Chapter \ref{Chapter data sets} covers the data sets that we used and the design of our experiments.
In chapter \ref{ChapterResults}, we present and analyze the results from the experiments.
In chapter \ref{ChapterConclusion}, we conclude the findings and discuss future work.