The first Shapelet Transform (TS) was introduced in \cite{hills2014classification}.
While the original algorithm embeded shapelets discovery in decision trees and assesed candidates through enumeration and 
the use Information Gain (IG) at each node, TS proposed a different way that saved repeating the brute force multiple times \cite{bostrom2018shapelet}.
TS seggregated the shapelets discovery process from the classifier. This seggregation opened the door for choosing classifiers freely and considering more accurate
classifiers than decision trees \cite{bagnall2017great,lines2015time}. Also \cite{hills2014classification} experimented with other shapelet assessment metrics like Kruskal-Wallis, F-stat and Moodâ€™s median
to find out that F-stat attained higher accuracies than the other three and than IG \cite{bostrom2018shapelet}.\newline
TS follows a three step procedure. In the beginning, a data transformation phase is carried out by utilizing a single-scan algorithm and extracting K best shapelets from the training
data set where K represents a cutoff threshold for the maximum number of shapelets to extract without affecting the quality of the shapelets extracted.
Then a reduction process is done by clustering the shapelets together untill they reach a user defined number.
Finally, The clustered shapelets are then used to transform the original datasest, by representing instances in terms of their disatances to each one of the extracted shapelets.
They experimented with different classifiers other than decision trees, these are; C4.5 tree, 1-NN, naive Bayes, Bayesian network, Random Forest, Rotation Forest, and support vector machine,
for which decision trees proved to be the worst among all, while support vector machine proved to be the best \cite{hills2014classification}.\newline
TS was then extended again by \cite{Bostrom2017}, the intuition behind it was that the previously used assessment technique couldn't hanlde multi-class problems \cite{Bostrom2017}.
Instead of assessing shapelets that discriminate between all classes, they accomodated a one-vs-all technique so that shapelets are assessed on their ability to separate one class to all other classes.
They also introduced a balancing technique to represent each of the classes with the same number of shapelets \cite{bagnall2017great}.
For the classification, a combination of tree based, kernel based and probabilistic classifiers were used in an ensemble on the transformed data set \cite{shifaz2020ts,lines2018time}.
Each of the classifiers was given a weight based on it's training accuracy and the final classification used weighted voting \cite{Bostrom2017}.
Although ST has proved to be a competent accurate classifier, it suffers from high training-time complexity \cite{shifaz2020ts}.