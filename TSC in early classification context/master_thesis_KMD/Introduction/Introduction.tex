Time Series Classification is a field of machine learning that has grabbed the attention of many reasearches in the last decade.
Time series data exists, by nature, in numerous real scenarios; medical examination records of patients\{reference\}, signal processing\{reference\}, weather forecasting\{reference\} and astronomy\{reference\} are some of them.\newline
Classification of time series data has been tackled with different objectives; the first is concerned with the accuracy of classification as well as space and time complexity. This objective is referred to simply as Time Series Classification (TSC). While the second objective adds the factor of earliness as a primary goal and is referred to as Early Time Series Classification (eTSC).\newline
Numerous algorithms have been introduced to tackle the problem of Time Series Classification. According to the \cite{bagnall2017great}, these algorithms can be divided, based on their technique, into six groups.\newline
Whole time series algorithms\{reference\} compare two time series, usually by employing an elastic distance measure between all data points of both time series.
Phase dependent interval algorithms\{reference\} operate by extracting informative features from intervals of time series, they are more suitable for long and noisy data than whole time series algorithms.
Phase independent interval algorithms\{reference\} are used when a class can be identified using a single or multiple patterns regardless of when they occur during the time series.
Dictionary based algorithms\{reference\} consider the number of repetitions of patterns as a factor of classification and not just simple occurence of one.
Ensembles\{reference\} combine the power of different algorithms, either of different or same core technique, then make the final classification decision based on voting.
In addition to the previous algorithms, there are also deep learning time series algorithms which build classifiers using generative as well as discriminative models.\newline
On the other hand, Early Time Series Classification algorithms are designed to deal with less data in order to achieve earliness of prediction, but of course this comes with a price of accuracy.
Many of the ideas applied in TSC have also been applied in eTSC; including 1-NN with Mimimum Prediction Length (MPL)\{reference\}, Phase independent intervals\{reference\}, generative classifiers\{reference\} and ensembles\{reference\}.\newline

Both, Time Series Classification Algorithms (TSCA) and Early Time Series Classification Algorithms (eTSCA), have introduced well performing algorithms in terms of their respective performance measures.
Their algorithms have been tested on publicly available archives\{reference\}; to benchmark their performance on a diverse set of datasets with different characteristics.\newline
According to \cite{bagnall2017great}, based on the "No free lunch theorem", no specific algorithm has proven to prevail over all others. This means that different problems with different datasets would require a choice between the algorithms based on how they perform on them, specially for non-public or non-experimented datasets.
In this thesis, we tackle this idea; by offering a framework that runs state-of-the-art algorithms on the provided dataset and provides analyses about the performance of each algorithm.\newline
Also due to their different objectives, TSCA and eTSCA have been dealt with as two different families. Which leaves studying the relationship between both algorithm families an open area for research.
We study the relationship between TSCA and eTSCA, by extending TSCA to deal with earliness as a main objective and compare how they perform in an early time series classification problem context.

% \begin{figure}[h]
%     \centering
% \includegraphics[width=11.2cm]{Figure/Signet_INF_1}
%     \caption{FIN-OVGU}
%     \label{fig:mesh1}
% \end{figure}


% \begin{table}[h!]
% \centering
% \begin{tabular}{||c c c c||} 
%  \hline
%  Col1 & Col2 & Col2 & Col3 \\ [0.5ex] 
%  \hline\hline
%  1 & 6 & 87837 & 787 \\ 
%  2 & 7 & 78 & 5415 \\
%  3 & 545 & 778 & 7507 \\
%  4 & 545 & 18744 & 7560 \\
%  5 & 88 & 788 & 6344 \\ [1ex] 
%  \hline
% \end{tabular}
% \caption{Table to test captions and labels}
% \label{table:1}
% \end{table}

\subsection*{Goals}
