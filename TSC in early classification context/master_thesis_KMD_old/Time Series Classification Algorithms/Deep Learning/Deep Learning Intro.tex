Deep learning is a long established group of machine learning algorithms which has proved competence in many areas \cite{lecun2015deep}
and which have encouraged the introduction of deep learning algorithms for time series classification \cite{wang2017time}.
Deep learning is appealing for investigating time series data; due to the role that the time dimension play as a structure for the data
and because deep learning algorithms can scale linearly with training size \cite{shifaz2020ts}.

The general framework of deep learning neural networks is described by \cite{fawaz2019deepreview} as an architecture of \emph{L} layers, each one of them is considered an input domain representation.
Each of the layers consists of small computing units referred to as neurons. Neurons compute elements for the output of each layer.
A layer $l\textsubscript{i}$ applies an activation function to it's input then passes the output to the next layer $l\textsubscript{i+1}$.
The behavior of activation functions in each layer is controlled by a set of parameters $\theta\textsubscript{i}$ and are referred to by the term weights.
The weights are assigned to the links between the inputs and outputs of the network's layers. A neural network carries out a series of computations to predict the class label for a given input \emph{x},
these calculation are noted as:
\begin{definition}
    $f\textsubscript{L}(\theta\textsubscript{L},x) = f\textsubscript{L-1}(\theta\textsubscript{L-1},f\textsubscript{L-2}(\theta\textsubscript{L-2},\ldots,f\textsubscript{1}(\theta\textsubscript{1},x)))$

    Where $f\textsubscript{i}$ represents the activation function applied at layer $l\textsubscript{i}$
\end{definition}
In the training phase of a neural network, the weights are randomly initialized. Then a forward pass of the input \emph{x} is done through the model and an output vector is computed,
in which every component of the vector represent a class probability. Using a cost function, the model's predicion loss is calculated
and weights of the network are updated, in a backward pass process, using a gradient descent.
By continuous interation of forward passes and weight updates by backward passes, the neural network learns the best weights to minimize the cost function.
For the prediction of unseen instances, or inference phase, the input data is passed through the network in a forward pass.
Then using the class probabilities of the output vector, the class with the highest probability is assigned.

Many of the deep learning research on time series focused on the use of variants of Convolutional Neural Networks (CNN). \cite{fawaz2019deep}
CNN model is based on the idea of learning convolutional filters that can accurately represent the data. Using these filters, CNNs can learn the hierarchical
structure of the data while incorporating translation invariance\cite{le2016data}.
Multi-Scale Neural Networks (MCNN) \cite{cui2016multi} and LeNet \cite{le2016data} were among the first experiments of using CNN in time series classification.
MCNN was composed of three stages; transformation stage, local convolution stage and full convolution stage. The transformation stage aimed at applying a low pass filter to exclude noise
and to capture different temporal patterns. Local convolution is a stage where different scale features are extracted and local features of the time series are learned.
In the final stage, the full convolution stage, all the features are concatenated and the final prediction is made.
On the other hand, LeNet consists of 2 convolutional layers, after each one a max pooling is carried out to do sub-sampling.
For the first layer, 5 filters are applied and the max pooling size is 2. While for the second there are 20 filters and max pooling size is 4.
In \cite{wang2017time} MultiLayer Perceptrons (MLP), Fully Convolutional Networks (FCN) and the Residual Networks (ResNet) were tested on 44 datasets from the
UCR time series archive, where FCN was not significantly different from state of the art and MCNN was not significantly different from COTE and BOSS \cite{schafer2017fast}.

A recent more comprehensive review of deep learning algorithms was done by \cite{fawaz2019deepreview}.
In which an experiment between nine deep learning algorithms was done.
These included the classical MCNN and LeNet, in addition to the three algorithms from the previously mentioned experiment; MLP, FCV and ResNet.
The other algorithms included were Encoder \cite{serra2018towards}, Multi Channel Deep Convolutional Neural Network (MCDCNN) \cite{zheng2014time,zheng2016exploiting},
Time Convolutional Neural Network \cite{zhao2017convolutional} and Time Warping Invariant Echo State Network (TWIESN) \cite{tanisaro2016time}.
For more details about the algorithms and their structures, please refer to \cite{fawaz2019deepreview}.

Finally, the newest deep learning algorithm introduced for time series classification is InceptionTime \cite{fawaz2020inceptiontime}, which is the newest state of the art
and which is an ensemble of of deep convolutional neural networks. It can attain high accuracy scores while maintaining scalability.
We will discuss in more details the InceptionTime algorithm in the next section.