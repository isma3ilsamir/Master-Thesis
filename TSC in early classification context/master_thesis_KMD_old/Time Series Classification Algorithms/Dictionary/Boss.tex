Bag of SFA Symbols (BOSS) is a dictionary based algorithm that was introduced by \cite{schafer2015boss} in 2015.
Like the previous dictionary based classifiers, Bag Of Patterns (BOP) and Symbolic Aggregate approXimation in Vector Space Model (SAXVSM),
BOSS also used a windowing technique to extract patterns from the data and transform them into words, but it had other significant differences to them \cite{bagnall2017great}.
BOSS was concerned with the issue of dealing with noisy data which, at that time, received little attention; due to the common practice of
either using raw data directly or handling noise in a preprocessing stage. The goal was to introduce an algorithm faster than it's rivals of the same group,
robust for existence of noise in the data and competitive with existing TSCA.

% General description of BOSS Model
The BOSS model is divided into several steps. In the beginning, it passes a window of size w over the time series to extract substructures.
Each of the extracted windows is then normalized to obtain amplitude invariance, while obtaining offset invariance by subtracting the mean value
for each window is data set specific and can be decided upon based on a parameter.
Then the substructures are quantized using SFA transformations which transforms the data into unordered words;
this helps further reduce the noise in the data by making it phase shift invariant and makes it possible to apply string matching algorithms on the data.
Since some data sets might happen to have long constant signals, this will lead to SFA transformations of their windows to produce the same words multiple times
and cause higher weights to be assigned for them. BOSS applies a numerosity reduction technique adopted from \cite{lin2007experiencing,lin2012rotation}
that ignores multiple sequential occurences of the same word.
Finally time series instances can be compared for differences, using the noise reduced substructures using a customized distance measure inspired by Euclidean distance.
We will, briefly, discuss the main components that are used for each of the previously mentioned steps.

% Windowing
To extract the substructures from a given time series instance $T = [t\textsubscript{1},t\textsubscript{2},...t\textsubscript{n}$],
a windowing function is used to split it into fixed sized windows $S\textsubscript{i;w} = (t\textsubscript{i},,...t\textsubscript{i+w-1}$), each of them is of a size \emph{w}.
The total number of windows that can be created for a time series of length n is \emph{n-w+1}, and each consecutive windows overlap on \emph{w-1} points.
In order to achieve offset and amplitude invariance, each window is z-normalized by subtracting the mean from it's original values and then dividing the difference
by the standard deviation.
\begin{definition}
    $windows(T,w) = \{ S\textsubscript{1;w}, S\textsubscript{2;w}, ...,  S\textsubscript{n-w+1;w} \}$
\end{definition}

%SFA
After running the windowing function, the real values of the time points inside the windows are transformed into words using Symbolic Fourier Approximation (SFA) \cite{schafer2012sfa}.
SFA is an alternative way of representing time series data, that instead of using real values, uses a sequence of symbols which are referred to as SFA words based on a predefined alphabet of specific length.
SFA accomplishes two things; low pass filtering through removal of noisy components from the data and string representation which allows for the use of string matching algorithms.\newline

%   SFA Step 1
For SFA to achieve it's goals, two main operations have to be carried out; approximation and quantization.
Approximation is the process of representing a specific signal of length n using another signal of lower length l.
This is achieved using Discrete Fourier Transformation (DFT); a transformation technique which is applied to a singal represented by a sequence of equally spaces values
into a sequence of coefficients of complex sinusoid waves ordered by their frequencies \cite{liao2017separable}. The higher coefficients in a DFT refer to data with
with rapid changes, which can be considered as noise in the signal and thus ignored. So considering only the first l $\ll$ n coefficients acts as a low pass filter for noise
producing a smoother signal.

%   SFA Step2
Quantization also helps reduce noise by splitting the frequency domain into equi-depth bins, then maps each of the Fourier real and imaginary coefficients into a bin.
BOSS utilizes Multiple Coefficient Binning (MCB), an adaptive technique that minimizes the loss of information which is a side effect of quantization.
After the approximation step, a matrix is is built from the Fourier transformations of the training data set using only the first $\frac{l}{2}$ coefficients,
including both the real and imaginary values for each coefficient. Then using an alphabet $\Sigma$ of size c, MCB creates for each columm of the matrix c+1 break points
using equi-depth binning. During classification, to acquire the SFA word for a Fourier transformed time series, a lookup is done on the precomputed MCB bins and
a word is assigned if the value falls within it's bin's boundaries.

% BOSS distance
After the transformation of instances to SFA words, BOSS uses a customized distnace measure referred to as BOSS distance; to measure the similarity between the transformed instances.
BOSS distance is a variation of Euclidean distance, which compares instances based on the histograms formed from their transformed forms. The appearence of the same SFA words in both
instances, is considered to be a notion of similarity. While the absence of SFA words might be caused by one of two reasons; the absence of some substructures from either instances,
or due to the presence of noise which disfigures the time series. Instances under BOSS distance are compared based on shared SFA words only, thus excluding words of frequencies equal to 0.
BOSS distance as noted by \cite{schafer2015boss} is:
\begin{definition}
    Given two time series instances $T\textsubscript{1}$ and $T\textsubscript{2}$ and their corresponding BOSS histograms $B\textsubscript{1}$ : $\Sigma\textsuperscript{l} \to \mathbb{N}$
    and $B\textsubscript{2}$ : $\Sigma\textsuperscript{l} \to \mathbb{N}$, where l is the word length and $\Sigma$ is an alphabet of size c. Their BOSS distance is:

    \centerline{D($T\textsubscript{1}$,$T\textsubscript{2}$) = dist($B\textsubscript{1}$,$B\textsubscript{2}$)}

    \centerline{dist($B\textsubscript{1}$,$B\textsubscript{2}$) = $\sum_{a \epsilon B\textsubscript{1};B\textsubscript{1}(a)>0}^{} [B\textsubscript{1}(a) - B\textsubscript{2}(a)]\textsuperscript{2}$}
\end{definition}

% Classifying with BOSS
Each single BOSS classifier utilizes 1-NN approach along with it's BOSS model. The reason behind using 1-NN is that it is a simple technique 
that doesn't add to the parameters of the model, but rather proved to be a robust one. When classifying a query isntance, BOSS searches
for the nearest neighbor in a sample of candidates by chossing the closest instance based on the BOSS distance.

Finally, BOSS Ensemble is introduced as an ensemble of multiple BOSS classifiers. While a fixed window length is used over time series instances for BOSS classifier,
BOSS Ensemble considers representing each time series instance by ensembling multiple BOSS classifiers, each of a different window length.
When the training data is fitted, BOSS Ensemble acquires a group of scores for each of the different window lengths.
To classify a query instance, BOSS Ensemble acquires the best accuracy score from the score sets returned during training.
Then considers all window lengths that obtained accuracies within a factor of the best accuracy score.
Each of the considered windows predicts a class label for the query instance using 1-NN. Majority voting is applied and the most dominant
class label is assigned.