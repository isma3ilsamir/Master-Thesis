Proximity forest was developed by \cite{lucas2019proximity}.
It was introduced as an addition to scalable time series classification, offering a more scalable and accurate classifier than EE \cite{tan2020fastee}.
On one side, EE was an accurate classifier being one the state of the art algorithms and the best among distnace based algorithms, as it combines 11 NN-algorithms each using a different elastic measure.
But on the other hand, EE's training process was very slow as it scales quadratically with the training size of the data set \cite{lines2015time,bagnall2017great}.
This goes back to the leave-one-out-cross-validation (LOOCV) used to optimize the parameters for each used metric \cite{shifaz2020ts}.\newline
Proximity Forest wanted to achieve two main goals. The first was to offer an adaptable algorithm that can scale with huge data sets consisting of millions of time series instances.
Beating EE, by orders of maginute, and other state of the art algorithms in terms of training and testing run time complexity.
While the other goal was to develop a competitive algorithm on the UCR data sets archive without the need to sacrifice accuracy for scalability as is the case with BOSS-VS \cite{lucas2019proximity}.\newline

Capitalizing on the previous research that has been put in developing specialized time series distance measures and inspired by the existing EE \cite{fawaz2020inceptiontime,fawaz2019deep}.
Proximity forests combine the the eleven elastic distances from EE along with a tree-based algorithms to form an ensemble of decision trees.
The reason behind using tree-based algorithms lies in the divide-and-conquer strategy that they adopt, which makes them scalable for large data sets.
Also a stochastic process is used for the selection of distance measures and their hyper-parameters, which usually hinders the performance of other algorithms,
like KNN, that need to learn the hyper-parameters of the utilized distance measure for each data set before using it \cite{lucas2019proximity}.
Proximity forests can scale sublinearly with training data set size, but quadratically with the length of the time series \cite{shifaz2020ts}.

%\subsection{Learning a proximity forest}
Proximity forests are based on a similar concept as Random Forests \cite{breiman2001random}, another tree-based ensemble, which learns only on a subset of the available features
for building tree nodes. This process insinuates in a factor of variability between the trees that form the ensemble but each with a low bias.
The collective classification accuracy of the ensemble then tends to provide better results than any if it's single classifiers \cite{lucas2019proximity}.

The building unit of a proximity forest is called the proximity tree. A proximity tree and a decision tree are similar on all aspects,
but they differ in the tests they apply in internal nodes.
A conventional decision tree builds it's nodes using attributes. When an instance is being tested, it is compared to the value of the attribute
and then follows the branch to which it conforms.\newline
Unlike conventional decision trees, that use feature values for their nodes, proximity trees build their nodes using randomly selected examplars.
When an instance to be tested, an elastic distance measure is calculated and then it follows the branch of the nearest examplar.\newline
An internal node in the tree holds two attributes; \emph{measure} and \emph{branches}.
As noted in \cite{lucas2019proximity}, a measure is function \emph{object} $\times$ \emph{object} $\rightarrow\mathbb{R}$.
Proximity Forest uses the same 11 distance measures used by EE; Euclidean distance (ED) Dynamic time warping using the full window (DTW);
Dynamic time warping with a restricted warping window (DTW-R); Weighted dynamic time warping (WDTW);
Derivative dynamic time warping using the full window (DDTW); Derivative dynamic time warping with a restricted warping window (DDTW-R);
Weighted derivative dynamic time warping (WDDTW); Longest common subsequence (LCSS); Edit distance with real penalty (ERP);
Time warp edit distance (TWE); and, Move-Split-Merge (MSM).
Proximity Forest saves a lot of the computational cost by replacing parameter searches with random sampling \cite{fawaz2020inceptiontime,fawaz2019deepreview}.
While branches is a vector of the possible branches to follow, each branch holding two attributes; \emph{examplar} and \emph{subtree}.
\emph{examplar} is a time series instance to which a query instance is compared, and \emph{subtree} refers to the tree an instance should follow
in case it is closest to a specific examplar.\newline
If all time series in a specific node share the same class, then a leaf node is created and the value of the class label is assigned to the \emph{class}
attribute of this node. During classification, if a query instance is to reach this node, it is directly labeled with the value of it's \emph{class} attribute.

%\subsection{Classifying with a proximity forest}
When a query time series is to be classified, it starts at the root node of a proximity tree.
The distance between the query and each of the randomly selected examplars is calculated, using the randomly selected distance measure at the node.
Then the query travels down the branch of the nearest examplar. This processes is repeated, passing through the internal nodes of the tree
till the query reaches a leaf node, where it is assigned the class label of that node. This whole process is then repeated for all the trees
constituting the forest. The final classification of the forest is made by majority voting between it's trees.